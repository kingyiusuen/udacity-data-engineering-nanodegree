# Data Warehouses

## Introduction

The purpose of this project is to build an ETL pipeline for a fictional music streaming startup, Sparkify. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app. The ETL pipeline extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights into what songs their users are listening to.

The project consists of three datasets:

1. The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song.

```json
{
  "num_songs": 1,
  "artist_id": "ARJIE2Y1187B994AB7",
  "artist_latitude": null,
  "artist_longitude": null,
  "artist_location": "",
  "artist_name": "Line Renaud",
  "song_id": "SOUPIRU12A6D4FA1E1",
  "title": "Der Kleine Dompfaff",
  "duration": 152.92036,
  "year": 0
}
```

2. The second dataset consists of log files in JSON format generated by an event simulator based on the songs in the dataset above.

![image](https://video.udacity-data.com/topher/2019/February/5c6c3ce5_log-data/log-data.png)

3. The third dataset contains the meta information that is required by AWS to correctly load the log files.

![image](https://video.udacity-data.com/topher/2022/May/6276f08b_log-json-path/log-json-path.png)

## Database Schemas

To optimize queries on song play analysis, a star schema is used in the data warehouse. This includes the following tables.

### Fact Table

1. songplays - records in event data associated with song plays i.e. records with page `NextSong`
   - songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

### Dimension Tables

1. users - users in the app
   - user_id, first_name, last_name, gender, level
2. songs - songs in music database
   - song_id, title, artist_id, year, duration
3. artists - artists in music database
   - artist_id, name, location, lattitude, longitude
4. time - timestamps of records in songplays broken down into specific units
   - start_time, hour, day, week, month, year, weekday

## Folder Structure

- `create_table.py`: Create fact and dimension tables for the star schema in Redshift
- `etl.py`: Load data from S3 into staging tables on Redshift and then process that data into your analytics tables on Redshift
- `sql_queries.py`: SQL statements which will be imported into the two other files above
- `dwh.cfg`: Configuration file
